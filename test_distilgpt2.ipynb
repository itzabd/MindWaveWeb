{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T07:49:11.885501Z",
     "start_time": "2025-05-04T07:44:05.837574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import os\n",
    "\n",
    "# =============================================\n",
    "# 1. ENVIRONMENT CONFIGURATION\n",
    "# =============================================\n",
    "torch.set_num_threads(os.cpu_count())\n",
    "print(f\"Using CPU with {os.cpu_count()} cores\")\n",
    "\n",
    "# =============================================\n",
    "# 2. MODEL LOADING WITH IMPROVED CONFIG\n",
    "# =============================================\n",
    "print(\"\\nLoading model...\")\n",
    "model_path = \"./fine_tuned_distilgpt2\"\n",
    "base_model = \"distilgpt2\"\n",
    "\n",
    "# Load components\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map={\"\": \"cpu\"}\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, model_path)\n",
    "model = model.merge_and_unload()\n",
    "model.eval()\n",
    "\n",
    "# =============================================\n",
    "# 3. OPTIMIZED DATA PROCESSING\n",
    "# =============================================\n",
    "def load_val_data(filepath):\n",
    "    data = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            # Use the exact prompt structure from training\n",
    "            input_text = (\n",
    "                \"EEG Features:\\n\"\n",
    "                f\"{entry['messages'][1]['content']}\\n\"\n",
    "                \"Label: \"\n",
    "            )\n",
    "            data.append({\n",
    "                \"input_text\": input_text,\n",
    "                \"true_label\": int(entry['messages'][2]['content']),\n",
    "                \"raw_eeg\": entry['messages'][1]['content'][:100] + \"...\"\n",
    "            })\n",
    "    return data\n",
    "\n",
    "val_data = load_val_data(\"jsonl/val.jsonl\")\n",
    "print(f\"\\nLoaded {len(val_data)} validation examples\")\n",
    "\n",
    "# =============================================\n",
    "# 4. IMPROVED PREDICTION HANDLING\n",
    "# =============================================\n",
    "def predict(text):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=256,  # Increased from 128\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # In predict() function, modify generation parameters:\n",
    "        outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=3,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    early_stopping=True,\n",
    "    num_beams=3,  # Add beam search\n",
    "    temperature=0.9  # Add temperature\n",
    ")\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract first valid digit in 1-5 range\n",
    "    for char in decoded.split(\"Label:\")[-1].strip():\n",
    "        if char.isdigit() and 1 <= int(char) <= 5:\n",
    "            return int(char), decoded\n",
    "    return None, decoded  # Explicit return for errors\n",
    "\n",
    "# =============================================\n",
    "# 5. SIMPLIFIED EVALUATION LOOP\n",
    "# =============================================\n",
    "results = []\n",
    "print(\"\\nRunning evaluation...\")\n",
    "\n",
    "for example in tqdm(val_data):\n",
    "    true_label = example[\"true_label\"]\n",
    "    pred_label, full_output = predict(example[\"input_text\"])\n",
    "\n",
    "    results.append({\n",
    "        \"true_label\": true_label,\n",
    "        \"predicted_label\": pred_label,\n",
    "        \"correct\": pred_label == true_label if pred_label else False,\n",
    "        \"eeg_preview\": example[\"raw_eeg\"],\n",
    "        \"full_output\": full_output\n",
    "    })\n",
    "\n",
    "# =============================================\n",
    "# 6. RESULTS ANALYSIS\n",
    "# =============================================\n",
    "# Filter valid predictions\n",
    "valid_results = [r for r in results if r[\"predicted_label\"] is not None]\n",
    "accuracy = sum(r[\"correct\"] for r in valid_results) / len(valid_results) if valid_results else 0\n",
    "\n",
    "print(f\"\\nValidation Accuracy (valid predictions): {accuracy:.2%}\")\n",
    "print(f\"Invalid predictions: {len(results) - len(valid_results)}/{len(results)}\")\n",
    "\n",
    "# Classification report for valid predictions\n",
    "if valid_results:\n",
    "    y_true = [r[\"true_label\"] for r in valid_results]\n",
    "    y_pred = [r[\"predicted_label\"] for r in valid_results]\n",
    "\n",
    "    print(\"\\nClassification Report (valid predictions):\")\n",
    "    print(classification_report(y_true, y_pred, labels=[1,2,3,4,5]))\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred, labels=[1,2,3,4,5]))\n",
    "# =============================================\n",
    "# MODIFIED EVALUATION SECTION\n",
    "# =============================================\n",
    "def evaluate(results):\n",
    "    y_true = [x['true_label'] for x in results]\n",
    "    y_pred = [x['predicted_label'] if x['predicted_label'] is not None else -1 for x in results]\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(\n",
    "        y_true, y_pred,\n",
    "        labels=[1,2,3,4,5],\n",
    "        target_names=[\"Class 1\", \"Class 2\", \"Class 3\", \"Class 4\", \"Class 5\"],\n",
    "        zero_division=0  # Add this parameter\n",
    "    ))\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(\n",
    "        y_true, y_pred,\n",
    "        labels=[1,2,3,4,5]\n",
    "    ))\n",
    "\n",
    "    # Add class presence diagnostics\n",
    "    present_classes = set(y_pred)\n",
    "    missing_classes = set([1,2,3,4,5]) - present_classes\n",
    "    if missing_classes:\n",
    "        print(f\"\\nWarning: No predictions for classes {missing_classes}\")\n",
    "# Save results\n",
    "with open(\"eeg_validation_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(\"\\nResults saved to eeg_validation_results.json\")"
   ],
   "id": "d4cc5dfeb36ecd83",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sakib\\PycharmProjects\\MindWaveWeb\\.venv2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU with 4 cores\n",
      "\n",
      "Loading model...\n",
      "\n",
      "Loaded 210 validation examples\n",
      "\n",
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/210 [00:00<?, ?it/s]C:\\Users\\sakib\\PycharmProjects\\MindWaveWeb\\.venv2\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 210/210 [04:50<00:00,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Accuracy (valid predictions): 17.62%\n",
      "Invalid predictions: 0/210\n",
      "\n",
      "Classification Report (valid predictions):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00        48\n",
      "           2       0.00      0.00      0.00        45\n",
      "           3       0.18      1.00      0.30        37\n",
      "           4       0.00      0.00      0.00        42\n",
      "           5       0.00      0.00      0.00        38\n",
      "\n",
      "    accuracy                           0.18       210\n",
      "   macro avg       0.04      0.20      0.06       210\n",
      "weighted avg       0.03      0.18      0.05       210\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 0  0 48  0  0]\n",
      " [ 0  0 45  0  0]\n",
      " [ 0  0 37  0  0]\n",
      " [ 0  0 42  0  0]\n",
      " [ 0  0 38  0  0]]\n",
      "\n",
      "Results saved to eeg_validation_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\sakib\\PycharmProjects\\MindWaveWeb\\.venv2\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\sakib\\PycharmProjects\\MindWaveWeb\\.venv2\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\sakib\\PycharmProjects\\MindWaveWeb\\.venv2\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T08:00:52.172498Z",
     "start_time": "2025-05-04T08:00:40.269169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %% [markdown]\n",
    "# ## Fine-Tuned DistilGPT-2 EEG Classification Test\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Setup Environment\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Import Dependencies\n",
    "\n",
    "# %%\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Configuration\n",
    "\n",
    "# %%\n",
    "MODEL_PATH = \"./fine_tuned_distilgpt2\"\n",
    "VAL_DATA = [\n",
    "    # Paste your validation JSONL entries here or load from file\n",
    "    # Example entry:\n",
    "    # {\"messages\": [{\"role\": \"system\", \"content\": \"...\"}, ...]}\n",
    "]\n",
    "\n",
    "# Use CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Load Model & Tokenizer\n",
    "\n",
    "# %%\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH).to(device)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set pad token\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Data Preparation\n",
    "\n",
    "# %%\n",
    "def parse_messages(entry):\n",
    "    \"\"\"Extract system, user, and assistant messages from JSON entry\"\"\"\n",
    "    roles = {'system': '', 'user': '', 'assistant': ''}\n",
    "    for msg in entry['messages']:\n",
    "        if msg['role'] in roles:\n",
    "            roles[msg['role']] = msg['content']\n",
    "    return roles\n",
    "\n",
    "# Prepare validation data\n",
    "validation_set = [parse_messages(entry) for entry in VAL_DATA]\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Prediction Function\n",
    "\n",
    "# %%\n",
    "def generate_prediction(system_prompt, user_input):\n",
    "    \"\"\"Generate model prediction for EEG features\"\"\"\n",
    "    prompt = f\"{system_prompt}\\n\\n{user_input}\\n\\nAssistant: \"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=3,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    # Extract generated text\n",
    "    input_length = inputs.input_ids.shape[1]\n",
    "    generated_tokens = outputs[:, input_length:]\n",
    "    return tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Evaluation\n",
    "\n",
    "# %%\n",
    "def evaluate_model(validation_data):\n",
    "    \"\"\"Run model evaluation on validation set\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    results = []\n",
    "\n",
    "    for item in tqdm(validation_data, desc=\"Processing\"):\n",
    "        true_label = int(item['assistant'])\n",
    "        generated_text = generate_prediction(item['system'], item['user'])\n",
    "\n",
    "        # Extract predicted number using regex\n",
    "        match = re.search(r'\\d+', generated_text)\n",
    "        predicted = int(match.group()) if match else None\n",
    "\n",
    "        # Validate prediction\n",
    "        is_correct = False\n",
    "        if predicted and 1 <= predicted <= 5:\n",
    "            is_correct = (predicted == true_label)\n",
    "            correct += int(is_correct)\n",
    "            total += 1\n",
    "        else:\n",
    "            print(f\"Invalid prediction: {generated_text}\")\n",
    "\n",
    "        results.append({\n",
    "            \"input\": item['user'],\n",
    "            \"true_label\": true_label,\n",
    "            \"predicted\": predicted,\n",
    "            \"correct\": is_correct\n",
    "        })\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy, results\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Run Evaluation\n",
    "\n",
    "# %%\n",
    "# Run evaluation\n",
    "accuracy, predictions = evaluate_model(validation_set)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Display Results\n",
    "\n",
    "# %%\n",
    "print(f\"\\nValidation Accuracy: {accuracy:.2%}\")\n",
    "print(\"\\nSample Predictions:\")\n",
    "for i, pred in enumerate(predictions[:5]):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"True: {pred['true_label']} | Predicted: {pred['predicted']}\")\n",
    "    print(f\"Correct: {'✓' if pred['correct'] else '✗'}\\n\")"
   ],
   "id": "49e6f20a6d46d696",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sakib\\PycharmProjects\\MindWaveWeb\\.venv2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Accuracy: 0.00%\n",
      "\n",
      "Sample Predictions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T07:01:54.808391Z",
     "start_time": "2025-05-04T07:01:48.496892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1. System Configuration\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "torch.set_num_threads(os.cpu_count())\n",
    "\n",
    "# 2. Model Loading\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map={\"\": \"cpu\"}\n",
    ")\n",
    "\n",
    "# 3. Corrected PEFT Configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"],  # DistilGPT-2 specific\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"lora_only\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\"lm_head\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# 4. Data Processing\n",
    "def preprocess_data(example):\n",
    "    return {\n",
    "        \"text\": f\"Analyze EEG features and predict movement class (1-5):\\n{example['messages'][1]['content']}\\nAnswer:\",\n",
    "        \"label\": int(example['messages'][2]['content'])\n",
    "    }\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"jsonl/train.jsonl\", split=\"train\")\n",
    "dataset = dataset.map(preprocess_data, remove_columns=[\"messages\"])\n",
    "\n",
    "# 5. Tokenization\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256,\n",
    "        return_tensors=\"np\"\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# 6. Training Configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./eeg_results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=3e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.05,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "# 7. Training\n",
    "trainer.train()\n",
    "model.save_pretrained(\"./fine_tuned_distilgpt2\")\n",
    "\n",
    "# 8. Evaluation\n",
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=256, truncation=True)\n",
    "    class_tokens = tokenizer(\"1 2 3 4 5\", add_special_tokens=False).input_ids[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            force_words_ids=[class_tokens],\n",
    "            bad_words_ids=[[tokenizer.eos_token_id]]\n",
    "        )\n",
    "    return int(tokenizer.decode(outputs[0][-1]))\n",
    "\n",
    "val_data = load_dataset(\"json\", data_files=\"jsonl/val.jsonl\", split=\"train\")\n",
    "val_data = val_data.map(preprocess_data, remove_columns=[\"messages\"])\n",
    "\n",
    "results = []\n",
    "for example in tqdm(val_data):\n",
    "    pred = predict(example[\"text\"])\n",
    "    results.append({\n",
    "        \"true\": example[\"label\"],\n",
    "        \"pred\": pred,\n",
    "        \"correct\": pred == example[\"label\"]\n",
    "    })\n",
    "\n",
    "accuracy = sum(r[\"correct\"] for r in results) / len(results)\n",
    "print(f\"Validation Accuracy: {accuracy:.2%}\")\n",
    "print(classification_report([r[\"true\"] for r in results], [r[\"pred\"] for r in results]))"
   ],
   "id": "a1533920158b42da",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target modules {'k_lin', 'out_lin', 'v_lin', 'q_lin'} not found in the base model. Please check the target modules and try again.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 44\u001B[39m\n\u001B[32m     33\u001B[39m \u001B[38;5;66;03m# 3. Corrected PEFT Configuration\u001B[39;00m\n\u001B[32m     34\u001B[39m peft_config = LoraConfig(\n\u001B[32m     35\u001B[39m     r=\u001B[32m16\u001B[39m,\n\u001B[32m     36\u001B[39m     lora_alpha=\u001B[32m32\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     41\u001B[39m     modules_to_save=[\u001B[33m\"\u001B[39m\u001B[33mlm_head\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m     42\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m44\u001B[39m model = \u001B[43mget_peft_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpeft_config\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     45\u001B[39m model.print_trainable_parameters()\n\u001B[32m     47\u001B[39m \u001B[38;5;66;03m# 4. Data Processing\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MindWaveWeb\\.venv2\\Lib\\site-packages\\peft\\mapping_func.py:123\u001B[39m, in \u001B[36mget_peft_model\u001B[39m\u001B[34m(model, peft_config, adapter_name, mixed, autocast_adapter_dtype, revision, low_cpu_mem_usage)\u001B[39m\n\u001B[32m    121\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m peft_config.is_prompt_learning:\n\u001B[32m    122\u001B[39m     peft_config = _prepare_prompt_learning_config(peft_config, model_config)\n\u001B[32m--> \u001B[39m\u001B[32m123\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001B[49m\u001B[43m[\u001B[49m\u001B[43mpeft_config\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtask_type\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    124\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    125\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpeft_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    126\u001B[39m \u001B[43m    \u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    127\u001B[39m \u001B[43m    \u001B[49m\u001B[43mautocast_adapter_dtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mautocast_adapter_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    128\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    129\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MindWaveWeb\\.venv2\\Lib\\site-packages\\peft\\peft_model.py:1722\u001B[39m, in \u001B[36mPeftModelForCausalLM.__init__\u001B[39m\u001B[34m(self, model, peft_config, adapter_name, **kwargs)\u001B[39m\n\u001B[32m   1719\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\n\u001B[32m   1720\u001B[39m     \u001B[38;5;28mself\u001B[39m, model: torch.nn.Module, peft_config: PeftConfig, adapter_name: \u001B[38;5;28mstr\u001B[39m = \u001B[33m\"\u001B[39m\u001B[33mdefault\u001B[39m\u001B[33m\"\u001B[39m, **kwargs\n\u001B[32m   1721\u001B[39m ) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1722\u001B[39m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpeft_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1723\u001B[39m     \u001B[38;5;28mself\u001B[39m.base_model_prepare_inputs_for_generation = \u001B[38;5;28mself\u001B[39m.base_model.prepare_inputs_for_generation\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MindWaveWeb\\.venv2\\Lib\\site-packages\\peft\\peft_model.py:132\u001B[39m, in \u001B[36mPeftModel.__init__\u001B[39m\u001B[34m(self, model, peft_config, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001B[39m\n\u001B[32m    130\u001B[39m     ctx = init_empty_weights \u001B[38;5;28;01mif\u001B[39;00m low_cpu_mem_usage \u001B[38;5;28;01melse\u001B[39;00m nullcontext\n\u001B[32m    131\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx():\n\u001B[32m--> \u001B[39m\u001B[32m132\u001B[39m         \u001B[38;5;28mself\u001B[39m.base_model = \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpeft_config\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    134\u001B[39m \u001B[38;5;28mself\u001B[39m.set_additional_trainable_modules(peft_config, adapter_name)\n\u001B[32m    136\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.base_model, \u001B[33m\"\u001B[39m\u001B[33m_cast_adapter_dtype\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MindWaveWeb\\.venv2\\Lib\\site-packages\\peft\\tuners\\lora\\model.py:142\u001B[39m, in \u001B[36mLoraModel.__init__\u001B[39m\u001B[34m(self, model, config, adapter_name, low_cpu_mem_usage)\u001B[39m\n\u001B[32m    141\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, model, config, adapter_name, low_cpu_mem_usage: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mFalse\u001B[39;00m) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m142\u001B[39m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MindWaveWeb\\.venv2\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:180\u001B[39m, in \u001B[36mBaseTuner.__init__\u001B[39m\u001B[34m(self, model, peft_config, adapter_name, low_cpu_mem_usage)\u001B[39m\n\u001B[32m    178\u001B[39m \u001B[38;5;28mself\u001B[39m._pre_injection_hook(\u001B[38;5;28mself\u001B[39m.model, \u001B[38;5;28mself\u001B[39m.peft_config[adapter_name], adapter_name)\n\u001B[32m    179\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m peft_config != PeftType.XLORA \u001B[38;5;129;01mor\u001B[39;00m peft_config[adapter_name] != PeftType.XLORA:\n\u001B[32m--> \u001B[39m\u001B[32m180\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minject_adapter\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapter_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    182\u001B[39m \u001B[38;5;66;03m# Copy the peft_config in the injected model.\u001B[39;00m\n\u001B[32m    183\u001B[39m \u001B[38;5;28mself\u001B[39m.model.peft_config = \u001B[38;5;28mself\u001B[39m.peft_config\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MindWaveWeb\\.venv2\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:527\u001B[39m, in \u001B[36mBaseTuner.inject_adapter\u001B[39m\u001B[34m(self, model, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001B[39m\n\u001B[32m    525\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(peft_config, \u001B[33m\"\u001B[39m\u001B[33mlayers_pattern\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    526\u001B[39m         error_msg += \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m You also specified \u001B[39m\u001B[33m'\u001B[39m\u001B[33mlayers_pattern\u001B[39m\u001B[33m'\u001B[39m\u001B[33m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpeft_config.layers_pattern\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m527\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(error_msg)\n\u001B[32m    528\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    529\u001B[39m     \u001B[38;5;66;03m# Some modules did not match and some matched but were excluded\u001B[39;00m\n\u001B[32m    530\u001B[39m     error_msg = (\n\u001B[32m    531\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mNo modules were targeted for adaptation. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    532\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mThis might be caused by a combination of mismatched target modules and excluded modules. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    533\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mPlease check your `target_modules` and `exclude_modules` configuration.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    534\u001B[39m     )\n",
      "\u001B[31mValueError\u001B[39m: Target modules {'k_lin', 'out_lin', 'v_lin', 'q_lin'} not found in the base model. Please check the target modules and try again."
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
